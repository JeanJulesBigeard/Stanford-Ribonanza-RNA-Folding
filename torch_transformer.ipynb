{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8252478",
   "metadata": {},
   "source": [
    "### Inports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67f7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f855e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example0'\n",
    "PATH = 'stanford-ribonanza-rna-folding-converted/'\n",
    "OUT = './'\n",
    "bs = 2\n",
    "num_workers = 4\n",
    "SEED = 2023\n",
    "nfolds = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c9565",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912e1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "    def __init__(self, df, mode='train', seed=2023, fold=0, nfolds=4, \n",
    "                 mask_only=False, **kwargs):\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP']\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP']\n",
    "        \n",
    "        split = list(KFold(n_splits=nfolds, random_state=seed, \n",
    "                shuffle=True).split(df_2A3))[fold][0 if mode=='train' else 1]\n",
    "                \n",
    "        df_2A3 = df_2A3.iloc[split].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.iloc[split].reset_index(drop=True)\n",
    "        \n",
    "        m = (df_2A3['SN_filter'].values > 0) & (df_DMS['SN_filter'].values > 0)\n",
    "        df_2A3 = df_2A3.loc[m].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.loc[m].reset_index(drop=True)\n",
    "        \n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_error_0' in c]].values\n",
    "        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        mask[:len(seq)] = True\n",
    "        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "                                               self.react_err_DMS[idx]],-1))\n",
    "        sn = torch.FloatTensor([self.sn_2A3[idx],self.sn_DMS[idx]])\n",
    "        \n",
    "        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n",
    "               {'react':react, 'react_err':react_err,\n",
    "                'sn':sn, 'mask':mask}\n",
    "    \n",
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n",
    "            else: L = s[\"mask\"].sum()\n",
    "            L = max(1,L // 16) \n",
    "            if len(buckets[L]) == 0:  buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "            \n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "                \n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "            \n",
    "def dict_to(x, device='cuda'):\n",
    "    return {k:x[k].to(device) for k in x}\n",
    "\n",
    "def to_device(x, device='cuda'):\n",
    "    return tuple(dict_to(e,device) for e in x)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dataloader, device='cuda'):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(dict_to(x, self.device) for x in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9344434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(PATH,'train_data.parquet'))\n",
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3928f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RNA_Dataset(df, mode='train', fold=fold, nfolds=nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ab4e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'seq': tensor([2, 2, 2, 0, 0, 1, 2, 0, 1, 3, 1, 2, 0, 2, 3, 0, 2, 0, 2, 3, 1, 2, 0, 0,\n",
      "        0, 0, 0, 1, 3, 3, 3, 2, 0, 3, 0, 3, 2, 2, 0, 3, 3, 3, 0, 1, 3, 1, 1, 2,\n",
      "        0, 2, 2, 0, 2, 0, 1, 2, 0, 0, 1, 3, 0, 1, 1, 0, 1, 2, 0, 0, 1, 0, 2, 2,\n",
      "        2, 2, 0, 0, 0, 1, 3, 1, 3, 0, 1, 1, 1, 2, 3, 2, 2, 1, 2, 3, 1, 3, 1, 1,\n",
      "        2, 3, 3, 3, 2, 0, 1, 2, 0, 2, 3, 0, 0, 2, 3, 1, 1, 3, 0, 0, 2, 3, 1, 0,\n",
      "        0, 1, 0, 3, 2, 1, 1, 0, 2, 2, 3, 0, 3, 3, 2, 0, 1, 3, 3, 1, 2, 2, 3, 1,\n",
      "        0, 0, 3, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'mask': tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])}, {'react': tensor([[        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [ 1.5600e-01,  1.8000e-02],\n",
      "        [ 4.3500e-01,  1.4520e+00],\n",
      "        [ 1.3800e-01,  1.8550e+00],\n",
      "        [ 1.6000e-01,  1.5600e-01],\n",
      "        [ 1.9400e-01,  1.5800e-01],\n",
      "        [-2.0000e-03,  1.3900e-01],\n",
      "        [-1.0000e-02,  2.1900e-01],\n",
      "        [ 0.0000e+00,  2.2900e-01],\n",
      "        [ 4.3000e-02,  7.9000e-01],\n",
      "        [ 2.7700e-01,  8.3300e-01],\n",
      "        [-2.7000e-02,  4.0600e-01],\n",
      "        [-7.3000e-02,  2.8000e-02],\n",
      "        [ 8.0000e-03, -2.0000e-01],\n",
      "        [ 2.6000e-02,  8.3000e-02],\n",
      "        [-2.4000e-02,  2.8400e-01],\n",
      "        [ 2.7000e-02,  1.7100e-01],\n",
      "        [ 2.8000e-02, -1.1500e-01],\n",
      "        [ 1.0000e-03, -3.5500e-01],\n",
      "        [-1.0500e-01,  3.3500e-01],\n",
      "        [-6.0000e-03,  7.4000e-02],\n",
      "        [ 1.5800e-01,  5.2000e-02],\n",
      "        [ 1.9800e-01,  1.3000e-01],\n",
      "        [ 5.6800e-01,  6.0000e-02],\n",
      "        [ 1.7000e-02,  2.0000e-03],\n",
      "        [ 1.0000e-03, -2.2400e-01],\n",
      "        [ 3.5000e-02,  2.2000e-01],\n",
      "        [ 4.4500e-01,  4.2500e-01],\n",
      "        [ 0.0000e+00,  1.7200e-01],\n",
      "        [ 6.1000e-02, -2.1700e-01],\n",
      "        [ 3.6000e-02,  2.3000e-01],\n",
      "        [ 2.7400e-01,  9.9200e-01],\n",
      "        [ 1.9600e-01,  3.4700e-01],\n",
      "        [ 7.9400e-01,  6.6500e-01],\n",
      "        [ 2.1350e+00,  1.2850e+00],\n",
      "        [ 8.3000e-01,  3.1120e+00],\n",
      "        [ 2.3150e+00,  1.4930e+00],\n",
      "        [ 9.0000e-03, -5.0400e-01],\n",
      "        [ 4.8000e-02,  2.1200e-01],\n",
      "        [ 5.2000e-02,  2.5400e-01],\n",
      "        [ 1.8400e-01,  1.9300e-01],\n",
      "        [ 4.9500e-01,  4.7500e-01],\n",
      "        [ 1.9300e-01,  1.6590e+00],\n",
      "        [ 3.1400e-01,  1.4040e+00],\n",
      "        [ 7.8200e-01,  1.3390e+00],\n",
      "        [ 1.9000e-02,  2.4900e-01],\n",
      "        [-2.9000e-02,  8.2000e-02],\n",
      "        [-1.6000e-02, -7.1500e-01],\n",
      "        [-1.3500e-01,  1.8900e-01],\n",
      "        [-4.3000e-02,  4.5900e-01],\n",
      "        [-4.3000e-02,  8.2100e-01],\n",
      "        [-1.8000e-02,  3.1500e-01],\n",
      "        [ 2.6000e-02,  1.1200e-01],\n",
      "        [ 5.8000e-02,  1.9400e-01],\n",
      "        [-2.1000e-02,  1.8500e-01],\n",
      "        [ 7.9000e-02, -6.5000e-02],\n",
      "        [ 3.0600e-01,  1.5210e+00],\n",
      "        [ 9.8000e-02,  1.5640e+00],\n",
      "        [ 1.9200e-01,  1.4880e+00],\n",
      "        [ 1.1000e-02,  2.3400e-01],\n",
      "        [-3.7000e-02,  1.3200e-01],\n",
      "        [ 1.3000e-02,  6.0000e-03],\n",
      "        [ 4.0000e-03, -4.4500e-01],\n",
      "        [-1.5000e-02, -2.5900e-01],\n",
      "        [ 3.5000e-02,  1.7600e-01],\n",
      "        [ 2.1000e-02, -8.4000e-02],\n",
      "        [-6.6000e-02,  3.1700e-01],\n",
      "        [-2.6000e-02, -2.3300e-01],\n",
      "        [-1.5000e-02, -3.5100e-01],\n",
      "        [ 2.5000e-02, -1.6000e-01],\n",
      "        [-1.9000e-02, -1.2500e-01],\n",
      "        [ 8.2000e-02,  2.0800e-01],\n",
      "        [ 4.0900e-01,  5.9000e-01],\n",
      "        [ 3.6000e-01, -9.3000e-02],\n",
      "        [ 5.6600e-01,  5.1500e-01],\n",
      "        [ 9.2000e-02,  1.6700e-01],\n",
      "        [ 3.2000e-02,  1.0720e+00],\n",
      "        [ 3.6000e-02,  5.4780e+00],\n",
      "        [-2.9000e-02, -1.8700e-01],\n",
      "        [-2.6000e-02,  4.4900e-01],\n",
      "        [-1.7200e-01,  2.8700e-01],\n",
      "        [ 1.6000e-02, -2.5100e-01],\n",
      "        [ 4.0000e-03,  7.1000e-02],\n",
      "        [ 3.0000e-03,  1.0600e-01],\n",
      "        [ 3.4000e-02,  1.9400e-01],\n",
      "        [-1.4500e-01,  4.5800e-01],\n",
      "        [-3.7000e-02,  9.8000e-02],\n",
      "        [-1.8000e-02,  1.4500e-01],\n",
      "        [-5.1000e-02,  1.5900e-01],\n",
      "        [ 3.0000e-03,  5.4000e-02],\n",
      "        [ 1.2000e-02,  1.5400e-01],\n",
      "        [ 6.0000e-03,  6.5000e-02],\n",
      "        [-6.0000e-03, -2.0400e-01],\n",
      "        [-1.5000e-02,  1.1500e-01],\n",
      "        [ 4.2000e-02,  7.4000e-02],\n",
      "        [ 9.9000e-02,  3.5000e-01],\n",
      "        [ 2.9800e-01,  2.0420e+00],\n",
      "        [ 9.2900e-01,  1.1740e+00],\n",
      "        [ 1.7880e+00,  7.9800e-01],\n",
      "        [ 1.0550e+00,  4.3000e-01],\n",
      "        [ 2.4100e-01,  1.0800e-01],\n",
      "        [ 2.6000e-01,  1.2200e-01],\n",
      "        [ 4.1600e-01,  1.1160e+00],\n",
      "        [ 1.0800e+00,  3.7800e-01],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan],\n",
      "        [        nan,         nan]]), 'react_err': tensor([[   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [0.0780, 0.0780],\n",
      "        [0.0490, 0.0490],\n",
      "        [0.1600, 0.1600],\n",
      "        [0.0190, 0.0190],\n",
      "        [0.0230, 0.0230],\n",
      "        [0.0200, 0.0200],\n",
      "        [0.0260, 0.0260],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0400, 0.0400],\n",
      "        [0.0520, 0.0520],\n",
      "        [0.0300, 0.0300],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0490, 0.0490],\n",
      "        [0.0430, 0.0430],\n",
      "        [0.0240, 0.0240],\n",
      "        [0.0260, 0.0260],\n",
      "        [0.0450, 0.0450],\n",
      "        [0.0490, 0.0490],\n",
      "        [0.0310, 0.0310],\n",
      "        [0.0230, 0.0230],\n",
      "        [0.0240, 0.0240],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0660, 0.0660],\n",
      "        [0.5060, 0.5060],\n",
      "        [0.0460, 0.0460],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0250, 0.0250],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0450, 0.0450],\n",
      "        [0.0230, 0.0230],\n",
      "        [0.0390, 0.0390],\n",
      "        [0.0230, 0.0230],\n",
      "        [0.0350, 0.0350],\n",
      "        [0.0440, 0.0440],\n",
      "        [0.0690, 0.0690],\n",
      "        [0.0540, 0.0540],\n",
      "        [0.0570, 0.0570],\n",
      "        [0.0250, 0.0250],\n",
      "        [0.0220, 0.0220],\n",
      "        [0.0200, 0.0200],\n",
      "        [0.0260, 0.0260],\n",
      "        [0.0500, 0.0500],\n",
      "        [0.0510, 0.0510],\n",
      "        [0.0510, 0.0510],\n",
      "        [0.0420, 0.0420],\n",
      "        [0.0220, 0.0220],\n",
      "        [0.0600, 0.0600],\n",
      "        [0.0380, 0.0380],\n",
      "        [0.0290, 0.0290],\n",
      "        [0.0400, 0.0400],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0170, 0.0170],\n",
      "        [0.0290, 0.0290],\n",
      "        [0.0180, 0.0180],\n",
      "        [0.0480, 0.0480],\n",
      "        [0.0530, 0.0530],\n",
      "        [0.0760, 0.0760],\n",
      "        [0.0540, 0.0540],\n",
      "        [0.0200, 0.0200],\n",
      "        [0.0370, 0.0370],\n",
      "        [0.0260, 0.0260],\n",
      "        [0.0600, 0.0600],\n",
      "        [0.2720, 0.2720],\n",
      "        [0.0400, 0.0400],\n",
      "        [0.0510, 0.0510],\n",
      "        [0.0260, 0.0260],\n",
      "        [0.0440, 0.0440],\n",
      "        [0.0610, 0.0610],\n",
      "        [0.0690, 0.0690],\n",
      "        [0.0460, 0.0460],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0390, 0.0390],\n",
      "        [0.0510, 0.0510],\n",
      "        [0.0370, 0.0370],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0600, 0.0600],\n",
      "        [0.0980, 0.0980],\n",
      "        [0.0440, 0.0440],\n",
      "        [0.0270, 0.0270],\n",
      "        [0.0270, 0.0270],\n",
      "        [0.0440, 0.0440],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0270, 0.0270],\n",
      "        [0.0220, 0.0220],\n",
      "        [0.0350, 0.0350],\n",
      "        [0.0120, 0.0120],\n",
      "        [0.0180, 0.0180],\n",
      "        [0.0210, 0.0210],\n",
      "        [0.0180, 0.0180],\n",
      "        [0.0350, 0.0350],\n",
      "        [0.0120, 0.0120],\n",
      "        [0.0460, 0.0460],\n",
      "        [0.0160, 0.0160],\n",
      "        [0.0120, 0.0120],\n",
      "        [0.0240, 0.0240],\n",
      "        [0.0560, 0.0560],\n",
      "        [0.0430, 0.0430],\n",
      "        [0.0360, 0.0360],\n",
      "        [0.0330, 0.0330],\n",
      "        [0.0380, 0.0380],\n",
      "        [0.0420, 0.0420],\n",
      "        [0.0450, 0.0450],\n",
      "        [0.0380, 0.0380],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan],\n",
      "        [   nan,    nan]]), 'sn': tensor([11.8240,  9.2910]), 'mask': tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])})\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf76684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([206])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['seq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d59703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46380297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [ 1.5600e-01,  1.8000e-02],\n",
       "        [ 4.3500e-01,  1.4520e+00],\n",
       "        [ 1.3800e-01,  1.8550e+00],\n",
       "        [ 1.6000e-01,  1.5600e-01],\n",
       "        [ 1.9400e-01,  1.5800e-01],\n",
       "        [-2.0000e-03,  1.3900e-01],\n",
       "        [-1.0000e-02,  2.1900e-01],\n",
       "        [ 0.0000e+00,  2.2900e-01],\n",
       "        [ 4.3000e-02,  7.9000e-01],\n",
       "        [ 2.7700e-01,  8.3300e-01],\n",
       "        [-2.7000e-02,  4.0600e-01],\n",
       "        [-7.3000e-02,  2.8000e-02],\n",
       "        [ 8.0000e-03, -2.0000e-01],\n",
       "        [ 2.6000e-02,  8.3000e-02],\n",
       "        [-2.4000e-02,  2.8400e-01],\n",
       "        [ 2.7000e-02,  1.7100e-01],\n",
       "        [ 2.8000e-02, -1.1500e-01],\n",
       "        [ 1.0000e-03, -3.5500e-01],\n",
       "        [-1.0500e-01,  3.3500e-01],\n",
       "        [-6.0000e-03,  7.4000e-02],\n",
       "        [ 1.5800e-01,  5.2000e-02],\n",
       "        [ 1.9800e-01,  1.3000e-01],\n",
       "        [ 5.6800e-01,  6.0000e-02],\n",
       "        [ 1.7000e-02,  2.0000e-03],\n",
       "        [ 1.0000e-03, -2.2400e-01],\n",
       "        [ 3.5000e-02,  2.2000e-01],\n",
       "        [ 4.4500e-01,  4.2500e-01],\n",
       "        [ 0.0000e+00,  1.7200e-01],\n",
       "        [ 6.1000e-02, -2.1700e-01],\n",
       "        [ 3.6000e-02,  2.3000e-01],\n",
       "        [ 2.7400e-01,  9.9200e-01],\n",
       "        [ 1.9600e-01,  3.4700e-01],\n",
       "        [ 7.9400e-01,  6.6500e-01],\n",
       "        [ 2.1350e+00,  1.2850e+00],\n",
       "        [ 8.3000e-01,  3.1120e+00],\n",
       "        [ 2.3150e+00,  1.4930e+00],\n",
       "        [ 9.0000e-03, -5.0400e-01],\n",
       "        [ 4.8000e-02,  2.1200e-01],\n",
       "        [ 5.2000e-02,  2.5400e-01],\n",
       "        [ 1.8400e-01,  1.9300e-01],\n",
       "        [ 4.9500e-01,  4.7500e-01],\n",
       "        [ 1.9300e-01,  1.6590e+00],\n",
       "        [ 3.1400e-01,  1.4040e+00],\n",
       "        [ 7.8200e-01,  1.3390e+00],\n",
       "        [ 1.9000e-02,  2.4900e-01],\n",
       "        [-2.9000e-02,  8.2000e-02],\n",
       "        [-1.6000e-02, -7.1500e-01],\n",
       "        [-1.3500e-01,  1.8900e-01],\n",
       "        [-4.3000e-02,  4.5900e-01],\n",
       "        [-4.3000e-02,  8.2100e-01],\n",
       "        [-1.8000e-02,  3.1500e-01],\n",
       "        [ 2.6000e-02,  1.1200e-01],\n",
       "        [ 5.8000e-02,  1.9400e-01],\n",
       "        [-2.1000e-02,  1.8500e-01],\n",
       "        [ 7.9000e-02, -6.5000e-02],\n",
       "        [ 3.0600e-01,  1.5210e+00],\n",
       "        [ 9.8000e-02,  1.5640e+00],\n",
       "        [ 1.9200e-01,  1.4880e+00],\n",
       "        [ 1.1000e-02,  2.3400e-01],\n",
       "        [-3.7000e-02,  1.3200e-01],\n",
       "        [ 1.3000e-02,  6.0000e-03],\n",
       "        [ 4.0000e-03, -4.4500e-01],\n",
       "        [-1.5000e-02, -2.5900e-01],\n",
       "        [ 3.5000e-02,  1.7600e-01],\n",
       "        [ 2.1000e-02, -8.4000e-02],\n",
       "        [-6.6000e-02,  3.1700e-01],\n",
       "        [-2.6000e-02, -2.3300e-01],\n",
       "        [-1.5000e-02, -3.5100e-01],\n",
       "        [ 2.5000e-02, -1.6000e-01],\n",
       "        [-1.9000e-02, -1.2500e-01],\n",
       "        [ 8.2000e-02,  2.0800e-01],\n",
       "        [ 4.0900e-01,  5.9000e-01],\n",
       "        [ 3.6000e-01, -9.3000e-02],\n",
       "        [ 5.6600e-01,  5.1500e-01],\n",
       "        [ 9.2000e-02,  1.6700e-01],\n",
       "        [ 3.2000e-02,  1.0720e+00],\n",
       "        [ 3.6000e-02,  5.4780e+00],\n",
       "        [-2.9000e-02, -1.8700e-01],\n",
       "        [-2.6000e-02,  4.4900e-01],\n",
       "        [-1.7200e-01,  2.8700e-01],\n",
       "        [ 1.6000e-02, -2.5100e-01],\n",
       "        [ 4.0000e-03,  7.1000e-02],\n",
       "        [ 3.0000e-03,  1.0600e-01],\n",
       "        [ 3.4000e-02,  1.9400e-01],\n",
       "        [-1.4500e-01,  4.5800e-01],\n",
       "        [-3.7000e-02,  9.8000e-02],\n",
       "        [-1.8000e-02,  1.4500e-01],\n",
       "        [-5.1000e-02,  1.5900e-01],\n",
       "        [ 3.0000e-03,  5.4000e-02],\n",
       "        [ 1.2000e-02,  1.5400e-01],\n",
       "        [ 6.0000e-03,  6.5000e-02],\n",
       "        [-6.0000e-03, -2.0400e-01],\n",
       "        [-1.5000e-02,  1.1500e-01],\n",
       "        [ 4.2000e-02,  7.4000e-02],\n",
       "        [ 9.9000e-02,  3.5000e-01],\n",
       "        [ 2.9800e-01,  2.0420e+00],\n",
       "        [ 9.2900e-01,  1.1740e+00],\n",
       "        [ 1.7880e+00,  7.9800e-01],\n",
       "        [ 1.0550e+00,  4.3000e-01],\n",
       "        [ 2.4100e-01,  1.0800e-01],\n",
       "        [ 2.6000e-01,  1.2200e-01],\n",
       "        [ 4.1600e-01,  1.1160e+00],\n",
       "        [ 1.0800e+00,  3.7800e-01],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan],\n",
       "        [        nan,         nan]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]['react']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbab83",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f28d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        pos = self.pos_enc(pos)\n",
    "        x = self.emb(x)\n",
    "        x = x + pos\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "\n",
    "        x = x + pos\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6e5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6d3d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = RNA_Dataset(df, mode='train', fold=fold, nfolds=nfolds)\n",
    "ds_train_len = RNA_Dataset(df, mode='train', fold=fold, \n",
    "            nfolds=nfolds, mask_only=True)\n",
    "sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs,\n",
    "            drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=num_workers,\n",
    "            persistent_workers=True), device)\n",
    "\n",
    "ds_val = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds)\n",
    "ds_val_len = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds, \n",
    "           mask_only=True)\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, \n",
    "           drop_last=False)\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "           batch_sampler=len_sampler_val, num_workers=num_workers), device)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17d6f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNA_Model()   \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11eb6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dl_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8078a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 177, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(data[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07222cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbdae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
